{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad3ad319",
   "metadata": {},
   "source": [
    "# Component 0 â€” Data Acquisition & Quality Filtering (The Data Pipeline)\n",
    "\n",
    "This initial notebook establishes the project's foundational data pipeline.\n",
    "\n",
    "Objective: To acquire a dataset of high-signal, relevant language-learning apps and apply aggressive filtering to ensure maximum data quality before modeling.\n",
    "\n",
    "**Key Steps & Deliverables:**\n",
    "\n",
    "1. Targeted Scraping: Used the [google_play_scraper API](https://pypi.org/project/google-play-scraper/) with expanded, high-signal search queries (e.g., 'pronunciation app', 'language tutor') to maximize the initial recall of relevant apps.\n",
    "\n",
    "2. Quality Filtering (Post-Acquisition): Applied a domain-specific keyword filter to the full text (title + description) to ensure every app is explicitly focused on language learning, removing irrelevant noise from the broader 'Education' category.\n",
    "\n",
    "3. Initial Feature Engineering: Created structured features essential for later modeling:\n",
    "\n",
    "4. realInstalls and score (cleaned and converted to numeric types).\n",
    "\n",
    "install_tier (A binned classification target, showing High/Medium/Low popularity).\n",
    "\n",
    "Final Output: A cleaned, unique dataset ready for the detailed Exploratory Data Analysis (EDA) in Notebook 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54320684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google_play_scraper import search, app\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- 1. CONFIGURATION AND KEYWORD DEFINITIONS ---\n",
    "\n",
    "# Expanded list of queries for higher recall during scraping\n",
    "SEARCH_QUERIES = [\n",
    "    'language learning', 'learn english', 'learn spanish', 'learn french',\n",
    "    'learn german', 'learn japanese', 'learn chinese', 'learn korean',\n",
    "    'learn italian', 'learn russian', 'learn portuguese', 'learn arabic',\n",
    "    'vocabulary', 'language tutor', 'language practice',\n",
    "    'pronunciation', 'language flashcards', 'language exchange',\n",
    "    'foreign languages', 'bilingual'\n",
    "]\n",
    "\n",
    "# Apps not specifically for language learning but potentially useful\n",
    "CATEGORY_QUERIES = {\n",
    "    'LLM': ['ai chatbot', 'chatgpt', 'gemini'], \n",
    "    'NOTEBOOK': ['note taking', 'notebook'], \n",
    "    'DICTIONARY&TRANSLATION': ['dictionary', 'translation']\n",
    "}\n",
    "\n",
    "# --- 2. DATA ACQUISITION FUNCTION ---\n",
    "\n",
    "def scrape_apps():\n",
    "    \"\"\"Scrapes app data using the google_play_scraper library.\"\"\"\n",
    "    app_data = []\n",
    "    scraped_app_ids = set()  # Track scraped app IDs for faster lookup\n",
    "    \n",
    "    print(f\"Starting scrape across {len(SEARCH_QUERIES)} main queries...\")\n",
    "    for query in SEARCH_QUERIES:\n",
    "        print(f\"  > Searching for: {query}\")\n",
    "        try:\n",
    "            results = search(\n",
    "                query,\n",
    "                lang='en',\n",
    "                country='us',\n",
    "                n_hits=1000\n",
    "            )\n",
    "            for res in results:\n",
    "                app_id = res['appId']\n",
    "                if app_id in scraped_app_ids:\n",
    "                    continue\n",
    "                try:\n",
    "                    details = app(app_id)\n",
    "                    details['category'] = 'LANGUAGE_LEARNING'  # Tag main query apps\n",
    "                    app_data.append(details)\n",
    "                    scraped_app_ids.add(app_id)\n",
    "                    time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            print(f\"Error during search for {query}: {e}\")\n",
    "    \n",
    "    # Scrape category-specific queries\n",
    "    for category, queries in CATEGORY_QUERIES.items():\n",
    "        print(f\"  > Searching for category: {category}\")\n",
    "        for query in queries:\n",
    "            print(f\"    > Query: {query}\")\n",
    "            try:\n",
    "                results = search(\n",
    "                    query,\n",
    "                    lang='en',\n",
    "                    country='us',\n",
    "                    n_hits=500\n",
    "                )\n",
    "                for res in results:\n",
    "                    app_id = res['appId']\n",
    "                    if app_id in scraped_app_ids:\n",
    "                        continue\n",
    "                    try:\n",
    "                        details = app(app_id)\n",
    "                        details['category'] = category\n",
    "                        app_data.append(details)\n",
    "                        scraped_app_ids.add(app_id)\n",
    "                        time.sleep(0.5)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "            except Exception as e:\n",
    "                print(f\"Error during search for {query}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(app_data)\n",
    "    df.drop_duplicates(subset=['appId'], inplace=True)\n",
    "    print(f\"Finished scraping. Total unique apps: {len(df)}.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape across 20 main queries...\n",
      "  > Searching for: language learning\n",
      "  > Searching for: learn english\n",
      "  > Searching for: learn spanish\n",
      "  > Searching for: learn french\n",
      "  > Searching for: learn german\n",
      "  > Searching for: learn japanese\n",
      "  > Searching for: learn chinese\n",
      "  > Searching for: learn korean\n",
      "  > Searching for: learn italian\n",
      "  > Searching for: learn russian\n",
      "  > Searching for: learn portuguese\n"
     ]
    }
   ],
   "source": [
    "# 1. Scrape data\n",
    "df = scrape_apps()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3be13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Final CLEANED DataFrame has 521 unique apps.\n",
      "Saved to language_apps_20251202_100340.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Save to CSV\n",
    "file_path= f'language_apps.csv'\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final CLEANED DataFrame has {len(df)} unique apps.\")\n",
    "print(f\"Saved to {file_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
